Processing Layer: "conv2d_6" (Conv2D)
InputSize = 3072
KernelSize = 1728
BiasSize = 64
Conv2D Output Size = 65536
First Channel output values "conv2d_6" layer:
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
5.30986 2.74243 3.25957 3.36544 3.22427 3.29858 3.18643 3.18018 3.33315 3.13804 3.02361 2.91747 2.98052 3.01125 3.08253 3.19622 3.38183 3.56947 3.47998 3.44485 3.43865 3.32181 3.31774 3.27749 3.12516 3.09753 3.14059 3.05955 2.97198 3.15497 3.20915 2.84659 
5.96696 3.58473 3.36107 3.16134 3.07991 3.01511 2.9493 2.87778 3.09526 3.50436 3.48581 3.17721 3.06637 2.30344 2.78036 2.43206 3.21953 3.34208 3.13499 3.40104 3.37579 3.51412 3.67653 3.85405 3.81584 3.62583 3.6165 3.20097 3.36587 3.17554 3.44302 2.9476 
5.42221 2.74992 2.93152 3.07372 3.00337 3.01774 3.19185 2.84863 2.88077 3.1634 3.17503 3.36142 3.49979 3.406 3.09809 2.91027 3.20064 3.15936 3.54128 3.55962 3.64041 3.71795 3.7794 3.89029 3.79825 3.82376 3.8408 4.32555 4.11251 3.98989 3.79073 3.02025 
5.91971 3.56333 3.24107 3.53998 3.41266 3.4857 3.5141 3.68778 3.68132 3.61779 3.64791 3.81259 3.02303 3.77933 2.23245 3.07485 4.03572 3.5733 3.61222 3.49159 3.5164 3.3489 3.39049 3.50703 3.69224 3.97343 4.00838 3.9362 3.86913 4.02938 3.83564 3.14448 
6.51069 4.70743 4.73418 4.16136 3.78922 3.63858 3.84451 4.03959 4.07634 3.94731 3.92035 3.84465 2.87104 2.11084 0 0.700666 2.90632 4.03912 3.96416 3.98172 4.01802 4.10711 4.02895 4.03 3.55961 3.36257 3.21021 3.09235 3.28403 3.53806 3.70072 3.44864 
5.73497 2.98964 2.91946 3.35398 3.08089 2.88447 2.85635 3.04033 3.30529 3.42987 3.28015 3.06911 5.87824 3.43723 0 0 0 3.61578 4.66596 4.37859 4.60892 4.24029 4.39226 3.91777 3.49191 3.08635 2.53728 2.25673 2.61398 2.63471 2.71947 2.52576 
5.88711 2.77871 2.93051 3.26693 3.32525 3.15221 2.6495 2.68966 2.93652 3.17489 3.34523 3.14479 2.97248 9.50858 14.7083 11.713 13.1749 2.60247 1.40066 2.16195 2.1236 2.69573 2.58843 3.05031 3.74478 4.44692 4.88098 5.01297 5.27091 4.98148 4.54996 3.4397 
6.79848 3.68672 3.3349 3.70113 3.65563 3.36375 3.02456 2.62722 2.64432 2.48298 2.42136 2.66673 2.8445 0 0 0 2.68235 2.59383 2.66959 2.01974 2.26415 3.23382 3.16201 3.58103 3.8009 4.18435 4.23944 4.22578 4.12594 4.09791 3.85581 3.09982 
6.50591 2.80694 2.50324 2.41044 2.87249 2.97399 3.05791 3.06747 3.10176 3.21349 3.13776 2.67982 6.47285 1.73336 0 0 0 4.1764 3.22442 2.14602 3.71651 3.66544 3.97083 3.62464 3.04627 2.38542 2.17584 2.04493 2.36997 2.44962 2.49488 2.11141 
6.1139 3.24036 2.84782 3.08097 3.11621 3.55928 3.53935 3.46219 3.31533 3.61051 3.69233 3.91283 3.40617 13.0537 7.46577 12.3124 6.58937 3.69625 3.53173 3.57651 4.09464 3.32451 3.6208 3.47481 3.37806 3.57081 3.57376 3.87572 3.88989 3.88279 4.19931 2.8684
6.34886 3.80654 3.85512 3.5161 3.4005 3.29589 3.48665 3.50862 3.50215 3.24082 3.07614 2.78561 5.08163 1.87774 1.53369 3.80656 2.85637 2.52511 1.60005 0 0 0 0 4.86612 4.83832 4.18159 3.89706 3.91915 4.01718 4.2065 4.18731 3.09179 
5.59795 3.14023 3.39114 3.52967 3.46529 3.40771 3.35439 3.22802 3.07417 2.90437 3.24949 4.26953 6.98011 3.23057 2.41481 3.55487 4.17944 4.31922 5.82821 9.17393 13.3332 9.51571 7.33047 4.07542 4.19114 4.52521 4.55221 4.66103 4.75294 4.56583 4.50772 3.62819
5.66048 3.00847 3.00368 2.80687 2.82404 2.73586 2.66056 2.5739 2.82423 2.90301 3.42239 3.25887 4.82352 3.39027 0.144153 0 0 0 0 9.89188 10.2404 11.8917 8.92299 2.97252 3.34028 3.37953 3.56923 3.74371 3.64886 3.70235 3.59225 3.25078
5.43314 3.11344 3.18128 3.18094 2.87217 2.76029 2.69699 2.59034 2.97047 3.03539 0.727586 1.65361 2.5633 1.23541 0 0 0 0 0 0 0 0 0 5.0567 4.47591 3.74705 3.63756 3.57793 3.86983 3.7749 3.68912 2.88048
5.33989 2.86728 2.97201 3.09467 3.08575 3.27081 3.23351 3.21019 2.93599 3.28755 8.36871 10.4991 18.0635 17.6676 16.9896 25.208 28.9349 21.2991 12.0751 0 1.8699 2.38947 6.45004 3.09847 3.62344 3.9783 3.68457 3.51097 3.35333 3.11532 3.09418 2.62015
5.42524 2.95706 3.11663 2.85084 2.79672 2.74658 2.77473 2.43614 2.59844 2.85042 3.53921 3.73907 6.34496 4.37654 1.01736 0 7.25679 22.8917 26.8205 21.1129 6.55728 5.56894 3.51208 3.16187 3.08048 3.06667 3.16944 3.57353 3.75545 4.24145 4.09708 3.34232
5.26846 3.02658 3.03589 3.21222 2.97757 2.942 2.6614 2.41043 2.40539 2.43881 0 0 0 0 1.43997 0 0 0 0 0 0 0.676443 1.91041 2.94432 3.03814 3.22582 3.40314 3.09783 3.26321 3.3179 3.79639 2.93434
5.54689 3.25891 3.06981 2.91415 2.82233 2.64196 2.74326 2.51816 2.84535 3.87899 7.98789 10.7755 8.37442 5.09022 0 0 0 1.65207 8.84509 13.9508 11.6549 9.0311 4.69274 3.65515 3.14452 3.21242 2.97365 2.92128 2.83215 3.08166 3.17553 2.60024
4.77201 2.29609 2.32911 2.55854 2.59057 2.75712 2.63695 2.85422 2.83014 3.66551 9.85443 17.4245 23.7151 24.6466 30.3936 20.1121 16.2781 5.4906 0 0 0 0.251926 2.7888 3.30756 3.39714 3.26685 3.27013 3.03552 2.99889 3.16649 3.03765 2.46859
5.26859 2.93874 3.05788 2.87624 2.65758 2.59774 3.17265 3.11104 3.41191 0.943714 0 0 0 0 0 3.791 9.31815 6.90459 0 0 0 2.46231 4.04327 3.05899 2.72777 2.86132 3.41164 3.49001 3.59294 3.63775 3.69296 2.59894
5.19265 3.04465 3.74389 4.02807 4.20855 4.1527 0 0 0 1.57019 5.12605 1.33339 1.36537 0 0 0 0.230853 5.96493 10.8153 3.3892 0 0 0 3.11421 4.54182 4.42697 4.52392 4.68926 4.50626 4.86349 4.83125 3.48109
0 0 0 0 0 0 0 0 0 0 0 0 8.13602 15.139 10.7368 2.18808 0 0 6.16097 13.4558 18.4435 3.16516 0 0 0 0 0 0 0 0 0 0
7.13169 4.70797 2.88132 1.31316 0 0 1.63937 1.85176 6.24513 0 0 0 0 0 0 4.57783 9.27335 13.1775 0 2.86184 0.397461 5.03853 5.59073 0 0 0 0 0 0.340624 2.08117 3.35307 0.881688
6.12831 6.20638 6.21594 6.87711 6.49916 15.3472 11.0077 17.6275 13.8986 17.2858 18.9895 15.0555 6.53174 0 0 0 0 0 0 0 5.91425 3.72109 2.57262 7.65299 3.56004 0.0598809 0 0 0 0 3.76662 2.98174
4.17551 3.43843 3.41582 5.92541 8.31336 12.5603 24.0181 31.7636 40.6914 37.0475 33.9398 26.6405 24.9319 23.8744 23.2582 14.5595 4.27818 0 0 0 0 0 0 0 0 0 0 0.97405 0 2.40045 3.95023 5.03056
4.15643 3.698 10.8498 16.7926 22.9957 11.3415 0.398465 0 0 0 6.13624 26.7924 31.5867 39.5358 37.0256 34.3887 28.7625 27.0975 23.8535 20.178 14.3856 6.96196 0 0 0 0 5.34609 4.40282 4.2197 5.06862 8.86835 8.65071
4.0317 4.57054 0 0 0 0 0 0.0555467 0 0 0 0 0 0 4.93565 11.7039 28.3982 37.4058 40.2679 42.1211 45.4893 44.8927 44.4214 44.8549 33.7019 27.2941 21.8465 24.2493 30.6426 27.7889 19.2893 8.43653
2.89162 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.41649 3.55123 0 0 0 0 0
5.53253 8.27545 13.6829 14.8138 14.0232 10.7974 7.8114 5.90234 8.68247 11.6583 13.6589 13.7301 14.247 16.7596 20.7353 21.6234 17.0987 10.4846 5.79157 3.28305 1.73846 0 0 0 0 0 0 0.961416 5.76365 9.6026 9.08149 4.06241
3.83967 2.99451 3.27753 4.14901 5.68806 9.99861 13.3674 15.5768 14.0447 12.7207 12.6057 14.5336 12.3955 8.63839 5.12719 5.15169 7.47409 11.7645 12.5027 12.7246 11.3228 10.7257 10.5676 9.26855 8.28213 6.53204 5.23887 4.75924 5.30838 5.95079 6.70513 3.57962
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
65536
=======================================================================
Processing Layer: "batch_normalization_6" (BatchNormalization)
Input size: 65536
Gamma size: 64
Beta size: 64
Moving Mean size: 64
Moving Variance size: 64
65536
65536
BatchNorm Output Size = 65536
First Channel output values "batch_normalization_6" layer:
-0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754
138.473 71.3441 84.8654 87.6335 83.9423 85.8853 82.953 82.7896 86.7893 81.6878 78.6959 75.9208 77.5693 78.3729 80.2365 83.2091 88.0621 92.9681 90.6282 89.7099 89.5478 86.4929 86.3864 85.3339 81.351 80.6286 81.7545 79.6357 77.346 82.1305 83.5472 74.0676 
155.653 93.367 87.5194 82.2971 80.168 78.4737 76.7532 74.883 80.5693 91.2659 90.7808 82.7121 79.814 59.8664 72.3359 63.2292 83.8185 87.0228 81.6081 88.5642 87.9042 91.5208 95.7673 100.409 99.4097 94.4416 94.1979 83.3333 87.6447 82.6683 89.6618 76.7087
141.41 71.54 76.2881 80.0061 78.1667 78.5425 83.0947 74.1208 74.9613 82.3509 82.655 87.5285 91.1461 88.694 80.6434 75.7325 83.3245 82.2454 92.231 92.7107 94.823 96.8504 98.457 101.356 98.9498 99.6168 100.062 112.737 107.166 103.961 98.7531 78.6081
154.418 92.8075 84.3818 92.1971 88.8681 90.7779 91.5204 96.0614 95.8926 94.2315 95.0191 99.3247 78.6809 98.4552 58.0103 80.0357 105.159 93.0682 94.0859 90.9317 91.5806 87.2011 88.2886 91.3356 96.178 103.53 104.444 102.557 100.803 104.993 99.9274 81.8563 
169.87 122.721 123.421 108.444 98.7137 94.7752 100.159 105.26 106.221 102.847 102.142 100.163 74.7069 54.8306 -0.359754 17.96 75.6293 105.248 103.288 103.747 104.696 107.025 104.982 105.009 92.7103 87.5585 83.5749 80.4931 85.5051 92.147 96.3997 89.8088
149.588 77.8079 75.9727 87.3339 80.1935 75.058 74.3227 79.1332 86.0609 89.318 85.4035 79.8855 153.333 89.5105 -0.359754 -0.359754 -0.359754 94.179 121.637 114.123 120.146 110.507 114.481 102.075 90.9402 80.3364 65.9803 58.645 67.9858 68.5277 70.744 65.679
153.565 72.2929 76.2619 85.0579 86.5826 82.0584 68.9144 69.9644 76.419 82.6514 87.105 81.8644 77.3591 248.253 384.204 305.891 344.112 67.6849 36.2621 56.1668 55.1643 70.1233 67.3176 79.394 97.5517 115.91 127.259 130.71 137.454 129.887 118.604 89.575
177.394 96.0337 86.8351 96.4105 95.221 87.5893 78.7207 68.3319 68.779 64.5606 62.9496 69.3651 74.0129 -0.359754 -0.359754 -0.359754 69.7732 67.4589 69.4397 52.4486 58.839 84.1921 82.3145 93.2704 99.019 109.045 110.485 110.128 107.518 106.785 100.455 80.6886
169.745 73.031 65.0902 62.6638 74.7448 77.3985 79.5929 79.8428 80.7392 83.6607 81.6805 69.7072 168.88 44.9609 -0.359754 -0.359754 -0.359754 108.837 83.9463 55.7503 96.8127 95.4773 103.462 94.4106 79.2885 62.0098 56.53 53.1074 61.6059 63.6884 64.8717 54.8454
159.495 84.3631 74.0998 80.1957 81.1172 92.7018 92.1807 90.163 86.3233 94.0412 96.1803 101.946 88.6984 340.944 194.841 321.563 171.927 96.2829 91.9814 93.1522 106.699 86.5633 94.3101 90.493 87.9636 93.0032 93.0804 100.975 101.346 101.16 109.436 74.6377
165.638 99.1665 100.437 91.5726 88.5501 85.8151 90.8027 91.3772 91.2079 84.3751 80.0695 72.4731 132.505 48.736 39.7402 99.1671 74.3232 65.6621 41.4753 -0.359754 -0.359754 -0.359754 -0.359754 126.87 126.144 108.973 101.533 102.111 104.674 109.624 109.122 80.4785
146.005 81.7451 88.3056 91.9274 90.2441 88.7387 87.3446 84.0406 80.0178 75.5784 84.6019 111.272 182.143 84.1072 62.7783 92.5863 108.916 112.571 152.025 239.503 348.251 248.439 191.304 106.197 109.222 117.957 118.663 121.508 123.911 119.019 117.5 94.5035
147.64 78.3 78.1749 73.0289 73.4779 71.1725 69.2036 66.9377 73.4831 75.5428 89.1224 84.8472 125.757 88.2826 3.40928 -0.359754 -0.359754 -0.359754 -0.359754
 258.275 267.387 310.563 232.942 77.3603 86.9757 88.002 92.9618 97.5238 95.0439 96.4425 93.5638 84.6356 
141.696 81.0447 82.8183 82.8094 74.7364 71.8112 70.1562 67.3677 77.3065 79.0041 18.6638 42.8756 66.6607 31.9414 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 131.853 116.668 97.6112 94.7484 93.1894 100.821 98.3392 96.0964 74.9536
139.258 74.6086 77.3467 80.5539 80.3206 85.1592 84.1841 83.5743 76.405 85.597 218.45 274.15 471.93 461.58 443.853 658.732 756.177 556.53 315.357 -0.359754 48.531 62.1156 168.284 80.6533 94.3793 103.657 95.9774 91.4387 87.3169 81.0937 80.5411 68.147
141.489 76.956 81.1282 74.1787 72.7637 71.4528 72.1888 63.3359 67.5795 74.1677 92.1769 97.4025 165.537 114.07 26.2404 -0.359754 189.377 598.171 700.892 551.66 171.088 145.247 91.4676 82.3109 80.1829 79.8218 82.5089 93.0743 97.8307 110.538 106.763 87.029
137.39 78.7736 79.0171 83.6275 77.4922 76.5623 69.2257 62.6638 62.5319 63.4057 -0.359754 -0.359754 -0.359754 -0.359754 37.2898 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 17.3266 49.5902 76.623 79.0758 83.9831 88.6193 80.6366 84.9607 86.3905 98.9012 76.362
144.67 84.8481 79.9039 75.8339 73.4333 68.7172 71.3659 65.4804 74.0352 101.061 208.493 281.379 218.599 132.73 -0.359754 -0.359754 -0.359754 42.8355 230.905 364.401 304.372 235.769 122.337 95.2083 81.8573 83.6326 77.3897 76.0204 73.6899 80.2137 82.668 67.6264
124.41 59.6742 60.5374 66.5361 67.3737 71.7282 68.5863 74.2671 73.6374 95.4792 257.295 455.224 619.699 644.054 794.316 525.493 425.249 143.198 -0.359754 -0.359754 -0.359754 6.22715 72.5566 86.1201 88.4622 85.0557 85.1415 79.0073 78.0496 82.4316 79.0631 64.1842
137.394 76.4771 79.592 74.8429 69.1258 67.5612 82.5928 80.982 88.8486 24.3147 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 98.7603 243.274 180.169 -0.359754 -0.359754 -0.359754 64.0201 105.356 79.621 70.9608 74.4528 88.8415 90.8905 93.5818 94.7534 96.1969 67.5925 
135.408 79.2462 97.5284 104.959 109.678 108.217 -0.359754 -0.359754 -0.359754 40.6947 133.667 34.5032 35.3394 -0.359754 -0.359754 -0.359754 5.67616 155.6 282.418 88.2548 -0.359754 -0.359754 -0.359754 81.0649 118.391 115.388 117.923 122.246 117.462 126.802 125.959 90.6573
-0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 212.366 395.467 280.367 56.85 -0.359754 -0.359754 160.726 351.458 481.867 82.397 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754
186.106 122.736 74.9757 33.9744 -0.359754 -0.359754 42.5036 48.0565 162.926 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 119.333 242.103 344.18 -0.359754 74.4664 10.0323 131.378 145.816 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 8.54624 54.0548 87.31 22.693
159.872 161.913 162.163 179.45 169.568 400.91 287.45 460.532 363.036 451.596 496.143 393.284 170.42 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 154.275 96.9323 66.9044 199.736 92.7216 1.2059 -0.359754 -0.359754 -0.359754 -0.359754 98.1228 77.6014
108.814 89.5419 88.9508 154.567 217.003 328.043 627.622 830.137 1063.56 968.29 887.034 696.187 651.514 623.863 607.752 380.315 111.498 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 25.1079 -0.359754 62.4028 102.924 131.17
108.315 96.3288 283.319 438.702 600.89 296.177 10.0586 -0.359754 -0.359754 -0.359754 160.079 700.157 825.511 1033.35 967.717 898.771 751.669 708.136 623.318 527.216 375.769 181.669 -0.359754 -0.359754 -0.359754 -0.359754 139.42 114.757 109.969 132.165 231.513 225.823
105.054 119.142 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 1.09258 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 128.688 305.652 742.143 977.657 1052.49 1100.94 1189.01 1173.41 1161.09 1172.42 880.815 713.276 570.842 633.665 800.826 726.214 503.981 220.223
75.245 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 88.9682 92.4911 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754
144.295 216.011 357.394 386.963 366.292 281.95 203.878 153.964 226.653 304.459 356.767 358.63 372.144 437.84 541.789 565.008 446.705 273.772 151.068 85.4793 45.0943 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 24.7776 150.337 250.711 237.086 105.857
100.033 77.9351 85.3351 108.121 148.361 261.065 349.145 406.914 366.854 332.238 329.232 379.639 323.735 225.501 133.696 134.337 195.059 307.236 326.538 332.34 295.687 280.075 275.943 241.977 216.186 170.428 136.617 124.076 138.434 155.23 174.954 93.2336
-0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754 -0.359754
65536
=======================================================================
Processing Layer: "max_pooling2d_6" (MaxPooling2D)
InputSize = 65536
MaxPool Output Size = 16384
First Channel output values "max_pooling2d_6" layer:
138.473 87.6335 85.8853 82.953 86.7893 78.6959 78.3729 83.2091 92.9681 90.6282 89.5478 86.3864 81.351 81.7545 82.1305 83.5472
155.653 87.5194 80.168 83.0947 91.2659 90.7808 91.1461 80.6434 87.0228 92.7107 96.8504 101.356 99.6168 112.737 107.166 98.7531
169.87 123.421 98.7137 105.26 106.221 102.142 98.4552 80.0357 105.248 103.747 107.025 105.009 103.53 104.444 104.993 99.9274 
153.565 87.3339 86.5826 79.1332 89.318 87.105 248.253 384.204 344.112 121.637 120.146 114.481 115.91 130.71 137.454 118.604
177.394 96.4105 95.221 79.8428 83.6607 81.6805 168.88 -0.359754 108.837 83.9463 96.8127 103.462 109.045 110.485 107.518 100.455
165.638 100.437 92.7018 92.1807 94.0412 101.946 340.944 321.563 171.927 93.1522 106.699 126.87 126.144 102.111 109.624 109.436
147.64 91.9274 90.2441 87.3446 80.0178 111.272 182.143 92.5863 112.571 258.275 348.251 232.942 117.957 121.508 123.911 117.5
141.696 82.8183 85.1592 84.1841 85.597 274.15 471.93 658.732 756.177 315.357 62.1156 168.284 116.668 95.9774 100.821 96.0964
141.489 83.6275 77.4922 72.1888 74.1677 97.4025 165.537 37.2898 598.171 700.892 171.088 91.4676 83.9831 93.0743 110.538 106.763 
144.67 79.9039 73.4333 74.2671 101.061 455.224 644.054 794.316 425.249 364.401 304.372 122.337 88.4622 85.1415 82.4316 82.668
137.394 104.959 109.678 82.5928 88.8486 133.667 35.3394 98.7603 243.274 282.418 64.0201 105.356 118.391 122.246 126.802 125.959
186.106 74.9757 -0.359754 48.0565 162.926 -0.359754 395.467 280.367 344.18 351.458 481.867 145.816 -0.359754 -0.359754 54.0548 87.31
161.913 179.45 400.91 830.137 1063.56 887.034 651.514 607.752 111.498 -0.359754 154.275 199.736 92.7216 25.1079 62.4028 131.17
119.142 438.702 600.89 10.0586 -0.359754 700.157 1033.35 967.717 977.657 1100.94 1189.01 1172.42 880.815 633.665 800.826 503.981
216.011 386.963 366.292 203.878 304.459 358.63 437.84 565.008 446.705 151.068 45.0943 -0.359754 88.9682 92.4911 250.711 237.086
100.033 108.121 261.065 406.914 366.854 379.639 323.735 134.337 307.236 332.34 295.687 275.943 216.186 136.617 155.23 174.954
16384
=======================================================================
Processing Layer: "conv2d_7" (Conv2D)
InputSize = 16384
KernelSize = 409600
BiasSize = 256
Conv2D Output Size = 32768
First Channel output values "conv2d_7" layer:
91.1272 0 0 0 0 0 0 0 0 0 0 0 0 0 91.8457 122.351
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
8.84696 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
7.19654 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
7.94409 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
66.6435 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
32768
=======================================================================
Processing Layer: "batch_normalization_7" (BatchNormalization)
Input size: 32768
Gamma size: 256
Beta size: 256
Moving Mean size: 256
Moving Variance size: 256
32768
32768
BatchNorm Output Size = 32768
First Channel output values "batch_normalization_7" layer:
266.003 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 268.104 357.284
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
25.4615 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
20.6366 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
22.822 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
194.427 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
32768
=======================================================================
Processing Layer: "max_pooling2d_7" (MaxPooling2D)
InputSize = 32768
MaxPool Output Size = 8192
First Channel output values "max_pooling2d_7" layer:
266.003 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 357.284
25.4615 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
22.822 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
194.427 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
-0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206 -0.40206
8192
=======================================================================
Processing Layer: "conv2d_8" (Conv2D)
InputSize = 8192
KernelSize = 1179648
BiasSize = 512
Conv2D Output Size = 16384
First Channel output values "conv2d_8" layer:
0 0 0 0 0 0 0 0
0 2.92361 0 0 0 0 0 0
0 156.51 0 0 0 0 0 0
0 71.9221 0 0 0 74.8226 25.353 0
0 95.6285 0 0 0 0 0 0
0 119.738 0 0 0 0 0 0
0 175.652 129.096 200.784 204.35 0.913944 71.983 7.9666
133.11 258.412 106.248 75.1932 118.177 21.1173 32.9995 111.894
16384
=======================================================================
Processing Layer: "batch_normalization_8" (BatchNormalization)
Input size: 16384
Gamma size: 512
Beta size: 512
Moving Mean size: 512
Moving Variance size: 512
16384
16384
BatchNorm Output Size = 16384
First Channel output values "batch_normalization_8" layer:
-0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249
-0.844249 0.329144 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249
-0.844249 61.971 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249
-0.844249 28.0217 -0.844249 -0.844249 -0.844249 29.1858 9.3312 -0.844249
-0.844249 37.5363 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249
-0.844249 47.2126 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249 -0.844249
-0.844249 69.6539 50.9683 79.7406 81.1719 -0.477437 28.0461 2.35315 
52.5795 102.87 41.7985 29.3346 46.586 7.6312 12.4001 44.0644
16384
=======================================================================
Processing Layer: "max_pooling2d_8" (MaxPooling2D)
InputSize = 16384
MaxPool Output Size = 4096
First Channel output values "max_pooling2d_8" layer:
0.329144 -0.844249 -0.844249 -0.844249
61.971 -0.844249 29.1858 9.3312
47.2126 -0.844249 -0.844249 -0.844249
102.87 79.7406 81.1719 44.0644
4096
=======================================================================
Processing Layer: "dense_4" (Dense)
Dense Output Size = 512
First Channel output values "dense_4" layer:
0 188.448 0 0 0 0 76.6861 0 0 91.1847
=======================================================================
Processing Layer: "dense_5" (Dense)
Dense Output Size = 10
First Channel output values "dense_5" layer:
0.919748 0 1.56429e-24 0 0 1.75384e-28 0 0.0802515 0 6.79457e-30
=======================================================================
Max value: 0.919748 at index 0
Predicted Class: airplane
Processing completed successfully!
